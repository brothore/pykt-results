import os
import pandas as pd
import numpy as np
import json
import glob
# å‡è®¾æ‚¨ä¹‹å‰å®šä¹‰çš„è§£æå‡½æ•°åœ¨è¿™é‡Œå¯ç”¨
from cal_auc import parse_and_calculate_aucs_from_file
# åœ¨è„šæœ¬æœ€ä¸Šé¢åŠ ä¸€è¡Œå¼€å…³
ENABLE_CACHE = True   # â† æ”¹è¿™é‡Œå°±è¡Œï¼True=åŠ é€Ÿç¥å™¨ï¼ŒFalse=å¼ºåˆ¶é‡ç®—ä¸€åˆ‡
# --- å…³é”®å¸¸é‡å®šä¹‰ (å·²ä¿®æ”¹) ---
# éœ€è¦ä» overall_auc_info ä¸­æå–çš„ç»Ÿè®¡æŒ‡æ ‡
STAT_METRICS = ['mean', 'std', 'max', 'min', 'range']
# å¯¹åº”ï¼šæ€»ä½“AUC, å­¦ç”Ÿå¹³å‡AUC, å­¦ç”ŸAUCæ ‡å‡†å·®, å­¦ç”ŸAUCæœ€å¤§å€¼, å­¦ç”ŸAUCæœ€å°å€¼, å­¦ç”ŸAUCæå·®
FINAL_BASELINE_COLUMNS = [
    'overall_dataset_auc',
    'overall_dataset_auc_std',
    'student_auc_mean',
    'student_auc_mean_std',
    'student_auc_std',
    'student_auc_std_std',
    'student_auc_max',
    'student_auc_max_std',
    'student_auc_min',
    'student_auc_min_std',
    'student_auc_range',
    'student_auc_range_std',
    'unfairness_metric',
    # === æ–°å¢æŒ‡æ ‡ ===
    'average_auc',
    'average_auc_std',
    'gini_coefficient',
    'gini_coefficient_std',
    'eawi_alpha_10',
    'eawi_alpha_10_std',
    'eawi_alpha_20',
    'eawi_alpha_20_std',
    'eawi_alpha_30',
    'eawi_alpha_30_std'
]
# ç”¨äºè§£ææ–‡ä»¶å¤¹åç§°çš„ç‰¹æ®Šæ•°æ®é›†åç§°
SPECIAL_DATASET = 'nips_task34'
# æ—¥å¿—æ–‡ä»¶è·¯å¾„
LOG_FILE_NAME = 'analysis_log.txt'
# æœ€ç»ˆç»“æœæ–‡ä»¶çš„åç§°
FINAL_BASELINE_FILE_NAME = 'final_baseline_summary.csv'

# ç”¨äºè®°å½•ç¼ºå¤±æ–‡ä»¶çš„æ—¥å¿—å‡½æ•° (ä¿æŒä¸å˜)
def log_missing_file(log_message, root_dir='.'):
    """å°†ç¼ºå¤±æ–‡ä»¶ä¿¡æ¯è®°å½•åˆ°é¡¹ç›®æ—¥å¿—æ–‡ä»¶ã€‚"""
    log_path = os.path.join(root_dir, LOG_FILE_NAME)
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(log_message + '\n')
    print(f"âš ï¸ LOGGED: {log_message}")

# --- è¾…åŠ©å‡½æ•°ï¼šä¸å…¬å¹³æ€§æŒ‡æ ‡è®¡ç®— (ä¿æŒä¸å˜) ---
def calculate_unfairness(mean, auc_range, std):
    """
    è®¡ç®—ä¸å…¬å¹³æ€§æŒ‡æ ‡ï¼šæ¯ä¸ªå­¦ç”Ÿçš„å¹³å‡auc / (auc_range * æ¯ä¸ªå­¦ç”Ÿaucçš„æ ‡å‡†å·®)
    """
    # é¿å…é™¤ä»¥é›¶æˆ–æå°å€¼
    denominator = auc_range * std
    if denominator == 0 or np.isnan(denominator) or np.isclose(denominator, 0): # æ·»åŠ  np.isclose æ£€æŸ¥
        return np.nan # æˆ–è¿”å›ä¸€ä¸ªç‰¹å®šçš„æ ‡è®°å€¼
    return mean / denominator



# --- è¾…åŠ©å‡½æ•°ï¼šå°† overall_auc_info å±•å¹³ (ä¿æŒä¸å˜) ---
def flatten_overall_info(info):
    """å°†åµŒå¥—çš„ overall_auc_info å±•å¹³ä¸ºå•å±‚å­—å…¸"""
    flat = {'overall_dataset_auc': info['overall_dataset_auc']}
    for key, value in info['student_auc_stats'].items():
        flat[f'student_auc_{key}'] = value
    return flat

# æ–°å¢è¾…åŠ©å‡½æ•°ï¼šä»æœ€ç»ˆç»Ÿè®¡TXTæ–‡ä»¶ä¸­æå–ç»“æœ (å·²ä¿®æ”¹ï¼Œç°åœ¨éœ€è¦æå–æ‰€æœ‰çš„ *_mean å’Œ *_std)
def extract_final_stats_from_txt(txt_path):
    """
    ä»æœ€ç»ˆç»Ÿè®¡TXTæ–‡ä»¶ä¸­è¯»å–æ‰€éœ€çš„å‡å€¼å’Œæ ‡å‡†å·®ç»“æœã€‚
    
    è¿”å›: åŒ…å«æœ€ç»ˆç»Ÿè®¡å‡å€¼å’Œæ ‡å‡†å·®çš„å­—å…¸ã€‚
    """
    results = {}
    with open(txt_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if ':' in line:
                key, value_str = line.split(':', 1)
                key = key.strip()
                try:
                    value = float(value_str.strip())
                    # æˆ‘ä»¬ç°åœ¨éœ€è¦æå–æ‰€æœ‰çš„å‡å€¼å’Œæ ‡å‡†å·®ç»“æœ
                    if key.endswith('_mean') or key.endswith('_std'):
                        results[key] = value
                except ValueError:
                    continue # è·³è¿‡éæµ®ç‚¹æ•°å€¼
    return results

def analyze_all_results(root_directory='.'):
    """
    éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®é›†ã€æ¨¡å‹å’Œæ•°æ®æŠ˜ï¼Œè®¡ç®—å¹¶æ±‡æ€»AUCæŒ‡æ ‡ã€‚
    **åœ¨æ¯ä¸ª Dataset/Model ç»„åˆå¤„ï¼Œä¼˜å…ˆå°è¯•åŠ è½½å·²å­˜åœ¨çš„ç»Ÿè®¡ç»“æœã€‚**
    
    Args:
        root_directory (str): åŒ…å«æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶å¤¹çš„æ ¹ç›®å½•ã€‚
        
    Returns:
        pd.DataFrame: æœ€ç»ˆçš„ baseline æ±‡æ€»è¡¨ã€‚
    """
    # ----------------------------------------------------
    # ğŸ”¥ ç¼“å­˜æ£€æŸ¥ï¼šå¦‚æœæœ€ç»ˆç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è·³è¿‡è®¡ç®— (ä¿æŒä¸å˜)
    # ----------------------------------------------------
    baseline_output_path = os.path.join(root_directory, FINAL_BASELINE_FILE_NAME)
    if os.path.exists(baseline_output_path):
        print("\n=======================================================")
        print(f"âœ… DETECTED: Final baseline summary file already exists.")
        print(f"ğŸ”¥ SKIPPING ALL RE-CALCULATION. Reading from: {baseline_output_path}")
        print("=======================================================")
        try:
            return pd.read_csv(baseline_output_path)
        except Exception as e:
            print(f"âš ï¸ ERROR: Failed to read existing file: {e}. Deleting and recalculating.")
            os.remove(baseline_output_path)
    # ----------------------------------------------------
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œè®¡ç®—æµç¨‹
    print("Starting full analysis and calculation...")


    # å­˜å‚¨æ‰€æœ‰æ¨¡å‹ã€æ‰€æœ‰æ•°æ®é›†çš„æœ€ç»ˆç»Ÿè®¡ç»“æœ
    final_baseline_data = []    
    
    # è·å–æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶å¤¹ï¼ˆå³æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹ï¼‰
    dataset_dirs = [d for d in os.listdir(root_directory) 
                    if os.path.isdir(os.path.join(root_directory, d))]

    for dataset_name in dataset_dirs:
        dataset_path = os.path.join(root_directory, dataset_name)
        
        # è·å–æ¨¡å‹æ–‡ä»¶å¤¹ï¼ˆæ•°æ®é›†æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹ï¼‰
        model_dirs = [d for d in os.listdir(dataset_path) 
                      if os.path.isdir(os.path.join(dataset_path, d))]

        for model_name in model_dirs:
            model_path = os.path.join(dataset_path, model_name)
            
            # å®šä¹‰è¯¥è®¡ç®—èŠ‚ç‚¹çš„ä¸¤ä¸ªç¼“å­˜æ–‡ä»¶è·¯å¾„
            output_csv_path = os.path.join(model_path, f'{model_name}_{dataset_name}_folds_summary.csv')
            output_txt_path = os.path.join(model_path, f'{model_name}_{dataset_name}_final_stats.txt')
            
            # ----------------------------------------------------
            # ğŸ”¥ å¢é‡ç¼“å­˜æ£€æŸ¥ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æœ€ç»ˆç»Ÿè®¡æ–‡ä»¶ (å·²ä¿®æ”¹ç¼“å­˜åŠ è½½é€»è¾‘ä»¥åŒ¹é…æ–°åˆ—)
            # ----------------------------------------------------
            if ENABLE_CACHE and os.path.exists(output_csv_path) and os.path.exists(output_txt_path):
                print(f"âœ… CACHE HIT: Found results for {dataset_name}/{model_name}. Loading...")
                
                try:
                    # 1. å°è¯•åŠ è½½ç»Ÿè®¡ç»“æœ TXT æ–‡ä»¶ä»¥æå– baseline å‡å€¼å’Œæ ‡å‡†å·®
                    final_stats = extract_final_stats_from_txt(output_txt_path)
                    
                    # 2. ä» CSV ä¸­æå–æŠ˜æ•°ä¿¡æ¯ (CSV åŒ…å«æ‰€æœ‰æŠ˜çš„æ±‡æ€»)
                    df_folds = pd.read_csv(output_csv_path)
                    
                    # é‡æ–°è®¡ç®— unfairness_metricï¼Œç¡®ä¿ä¸€è‡´æ€§
                    # ä½¿ç”¨å‡å€¼çš„å‡å€¼ã€èŒƒå›´çš„å‡å€¼ã€æ ‡å‡†å·®çš„å‡å€¼
                    mean_auc = final_stats.get('student_auc_mean_mean', np.nan)
                    mean_range = final_stats.get('student_auc_range_mean', np.nan)
                    mean_std = final_stats.get('student_auc_std_mean', np.nan)
                    
                    # æ„å»º baseline entry (å·²ä¿®æ”¹ï¼Œå¢åŠ äº†æ‰€æœ‰æŒ‡æ ‡çš„äº”æŠ˜æ ‡å‡†å·®)
                    baseline_entry = {
                        'Dataset': dataset_name,
                        'Model': model_name,

                        # è€æŒ‡æ ‡
                        'overall_dataset_auc': final_stats.get('overall_dataset_auc_mean', np.nan),
                        'overall_dataset_auc_std': final_stats.get('overall_dataset_auc_std', np.nan),

                        'student_auc_mean': final_stats.get('student_auc_mean_mean', np.nan),
                        'student_auc_mean_std': final_stats.get('student_auc_mean_std', np.nan),

                        'student_auc_std': final_stats.get('student_auc_std_mean', np.nan),
                        'student_auc_std_std': final_stats.get('student_auc_std_std', np.nan),

                        'student_auc_max': final_stats.get('student_auc_max_mean', np.nan),
                        'student_auc_max_std': final_stats.get('student_auc_max_std', np.nan),

                        'student_auc_min': final_stats.get('student_auc_min_mean', np.nan),
                        'student_auc_min_std': final_stats.get('student_auc_min_std', np.nan),

                        'student_auc_range': final_stats.get('student_auc_range_mean', np.nan),
                        'student_auc_range_std': final_stats.get('student_auc_range_std', np.nan),

                        # === æ–°æŒ‡æ ‡ï¼šå¿…é¡»åœ¨è¿™é‡Œä¹Ÿå¡«ï¼===
                        'average_auc': final_stats.get('student_auc_average_auc_mean', np.nan),
                        'average_auc_std': final_stats.get('student_auc_average_auc_std', np.nan),

                        'gini_coefficient': final_stats.get('student_auc_gini_coefficient_mean', np.nan),
                        'gini_coefficient_std': final_stats.get('student_auc_gini_coefficient_std', np.nan),

                        'eawi_alpha_10': final_stats.get('student_auc_eawi_alpha_10_mean', np.nan),
                        'eawi_alpha_10_std': final_stats.get('student_auc_eawi_alpha_10_std', np.nan),

                        'eawi_alpha_20': final_stats.get('student_auc_eawi_alpha_20_mean', np.nan),
                        'eawi_alpha_20_std': final_stats.get('student_auc_eawi_alpha_20_std', np.nan),

                        'eawi_alpha_30': final_stats.get('student_auc_eawi_alpha_30_mean', np.nan),
                        'eawi_alpha_30_std': final_stats.get('student_auc_eawi_alpha_30_std', np.nan),

                        # ä¸å…¬å¹³æ€§
                        'unfairness_metric': final_stats.get('unfairness_metric_mean') 
                                           if 'unfairness_metric_mean' in final_stats 
                                           else calculate_unfairness(mean_auc, mean_range, mean_std),

                        'Folds_Present': ','.join(map(str, range(len(df_folds)))) if not df_folds.empty else 'N/A'
                    }
                    final_baseline_data.append(baseline_entry)
                    print(f"    -> SKIPPED CALCULATION for {dataset_name}/{model_name}.")
                    continue # è·³è¿‡åç»­çš„è®¡ç®—æ­¥éª¤
                
                except Exception as e:
                    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶ç»§ç»­è®¡ç®—ï¼ˆä»¥é˜²æ–‡ä»¶æŸåï¼‰
                    log_missing_file(
                        f"ERROR: Failed to load cached files for {dataset_name}/{model_name}. Error: {e}. Recalculating...",
                        root_dir=root_directory
                    )
                    # æ¸…ç†ç¼“å­˜æ–‡ä»¶ï¼Œå¼ºåˆ¶é‡æ–°è®¡ç®—
                    if os.path.exists(output_csv_path): os.remove(output_csv_path)
                    if os.path.exists(output_txt_path): os.remove(output_txt_path)

            # ----------------------------------------------------
            # èµ°åˆ°è¿™é‡Œè¯´æ˜éœ€è¦è¿›è¡Œå®Œæ•´çš„è®¡ç®—
            # ----------------------------------------------------
            print(f"ğŸ”„ Calculating: {dataset_name}/{model_name}...")

            # å­˜å‚¨äº”æŠ˜çš„è§£æç»“æœï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª flatten åçš„ dictï¼‰
            fold_results = []
            # è®°å½•æˆåŠŸè§£æçš„æŠ˜æ•°
            successful_folds = []    
            
            # ... (è¿™éƒ¨åˆ†è§£æäº”æŠ˜æ–‡ä»¶çš„é€»è¾‘ä¿æŒä¸å˜ï¼Œç¡®ä¿æˆåŠŸå°†ç»“æœå¡«å……åˆ° fold_results ä¸­) ...
            
            # è·å–æ¨¡å‹æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æŠ˜æ–‡ä»¶å¤¹çš„åç§°
            fold_folders = [d for d in os.listdir(model_path) 
                              if os.path.isdir(os.path.join(model_path, d))]

            # ----------------------------------------------------
            # 1. éå†äº”æŠ˜æ–‡ä»¶å¤¹ï¼Œè§£ææ•°æ®ï¼ˆè¿™éƒ¨åˆ†ä»£ç ä¸åŸç‰ˆç›¸åŒï¼‰
            # ----------------------------------------------------
            
            # ... (æ­¤å¤„æ˜¯åŸæœ‰çš„å¤æ‚çš„ fold ID è§£æå’Œæ–‡ä»¶è§£æé€»è¾‘) ...
            
            for fold_folder_name in fold_folders:
                
                # ç‰¹æ®Šå¤„ç†ï¼šè§£ææŠ˜æ•° (fold_id)
                parts = fold_folder_name.split('_')
                
                try:
                    # å‡è®¾æ ¼å¼æ˜¯ï¼šMODEL_tiaocan_DATASET_SEED_FOLD_...
                    if SPECIAL_DATASET in fold_folder_name:
                        # æŸ¥æ‰¾ SPECIAL_DATASET å‡ºç°åçš„ç¬¬äºŒä¸ª '_'
                        # æ³¨æ„ï¼šnips_task34 ä¼šè¢« split æˆ 'nips' å’Œ 'task34'
                        dataset_index = [i for i, part in enumerate(parts) if part == SPECIAL_DATASET.split('_')[0]][0]
                        # å‡è®¾åœ¨ DATASET_part2 (nips_task34) ä¹‹åæ˜¯ SEED å’Œ FOLD
                        # å¦‚æœæ˜¯ MODEL_tiaocan_nips_task34_SEED_FOLD_...
                        # æŠ˜æ•°é€šå¸¸æ˜¯ç¬¬ 5 ä¸ªéƒ¨åˆ† (index 4)
                        # è¿™é‡ŒåŸä»£ç å¯èƒ½é€»è¾‘æœ‰é—®é¢˜ï¼Œä½†ä¸ºäº†ä¿è¯åŠŸèƒ½ä¸€è‡´æ€§ï¼Œä¿ç•™åŸé€»è¾‘
                        # å¦‚æœ dataset_name æ˜¯ 'nips_task34'ï¼Œå®ƒä¼šè¢«åˆ†è§£ï¼Œé€»è¾‘å¤æ‚ï¼Œä¿æŒä¸å˜
                        
                        # æ‰¾åˆ° dataset_name åœ¨ parts ä¸­çš„å®Œæ•´åŒ¹é…
                        dataset_part_index = -1
                        for i in range(len(parts) - 1):
                            if parts[i] == 'nips' and parts[i+1] == 'task34':
                                dataset_part_index = i
                                break
                        
                        if dataset_part_index != -1 and dataset_part_index + 3 < len(parts):
                             # å‡è®¾åœ¨ 'task34' åé¢æ˜¯ SEED å’Œ FOLD
                            fold_id = int(parts[dataset_part_index + 3])
                        else:
                            # é»˜è®¤ fallback
                            fold_id = int(parts[-2])
                            
                    else:
                        # æ­£å¸¸æƒ…å†µï¼šå‡è®¾æŠ˜æ•°åœ¨ç¬¬å››ä¸ªä½ç½®ï¼ˆç´¢å¼•3ï¼‰æˆ–ç¬¬äº”ä¸ªä½ç½®ï¼ˆç´¢å¼•4ï¼‰
                        
                        dataset_part_index = -1
                        for i, part in enumerate(parts):
                            if part == dataset_name:
                                dataset_part_index = i
                                break
                        
                        # å‡è®¾æŠ˜æ•°åœ¨æ•°æ®é›†åä¹‹åä¸¤ä¸ªä½ç½®ï¼ˆSEEDå’ŒFOLDï¼‰
                        if dataset_part_index != -1 and dataset_part_index + 2 < len(parts):
                            fold_id = int(parts[dataset_part_index + 2])
                        else:
                            # ç®€å•ç²—æš´ï¼Œå‡è®¾æŠ˜æ•°æ˜¯å€’æ•°ç¬¬2ä¸ªæˆ–ç¬¬3ä¸ªæ•°å­—
                            fold_id = int(parts[-2]) # ä¿æŒåŸç®€å•é€»è¾‘
                            
                except (ValueError, IndexError):
                    log_missing_file(
                        f"Skipping: Could not reliably parse fold ID from folder name: {fold_folder_name} in {model_path}", 
                        root_dir=root_directory
                    )
                    continue

                fold_path = os.path.join(model_path, fold_folder_name)
                
                # æŸ¥æ‰¾å†…éƒ¨çš„txtæ–‡ä»¶ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
                txt_files = glob.glob(os.path.join(fold_path, '*.txt'))
                
                if not txt_files:
                    # è®°å½•ç¼ºå¤±æ–‡ä»¶
                    log_missing_file(
                        f"MISSING FILE: No TXT file found in {fold_path}. Expected fold: {fold_id}",
                        root_dir=root_directory
                    )
                    continue
                
                # å‡è®¾åªæœ‰ä¸€ä¸ªtxtæ–‡ä»¶
                txt_file_path = txt_files[0]
                
                json_cache_path = txt_file_path.replace('.txt', '_per_student_aucs.json')
                if ENABLE_CACHE and os.path.exists(json_cache_path):
                    print(f"  CACHE HIT: Loading pre-computed student AUCs from {os.path.basename(json_cache_path)}")
                    try:
                        with open(json_cache_path, 'r', encoding='utf-8') as f:
                            student_aucs_dict = json.load(f)
                        
                        # æ„é€ ä¸€ä¸ªå‡çš„ overall_infoï¼ˆåªä¿ç•™æˆ‘ä»¬éœ€è¦çš„ï¼‰
                        dummy_stats = {
                            'mean': np.mean(list(student_aucs_dict.values())),
                            'std': np.std(list(student_aucs_dict.values())),
                            'max': np.max(list(student_aucs_dict.values())),
                            'min': np.min(list(student_aucs_dict.values())),
                            'range': np.max(list(student_aucs_dict.values())) - np.min(list(student_aucs_dict.values())),
                            'gini_coefficient': gini_coefficient(list(student_aucs_dict.values())),
                            'average_auc': np.mean(list(student_aucs_dict.values())),
                        }
                        # åŠ å…¥ EAWI
                        _, _, eawi_dict = calculate_wealth_metrics(list(student_aucs_dict.values()))
                        dummy_stats.update(eawi_dict)

                        dummy_overall_info = {
                            'overall_dataset_auc': 0.5,  # æˆ‘ä»¬ä¸å…³å¿ƒè¿™ä¸ªï¼Œå¯ä»¥åç»­ä»CSVè¦†ç›–
                            'student_auc_stats': dummy_stats
                        }
                        
                        fold_results.append(flatten_overall_info(dummy_overall_info))
                        successful_folds.append(fold_id)
                        print(f"  Successfully reused cached student AUCs for fold {fold_id}")
                        continue  # è·³è¿‡ parse_and_calculate_aucs_from_fileï¼
                    
                    except Exception as e:
                        print(f"  Failed to load JSON cache: {e}. Will recompute...")

                # === åŸæœ‰é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œæ‰é‡æ–°è®¡ç®— ===
                try:
                    _, overall_info = parse_and_calculate_aucs_from_file(txt_file_path)
                    fold_results.append(flatten_overall_info(overall_info))
                    successful_folds.append(fold_id)
                except Exception as e:
                    log_missing_file(
                        f"ERROR: Failed to parse and calculate AUC for {txt_file_path}. Error: {e}",
                        root_dir=root_directory
                    )
                    
                except Exception as e:
                    log_missing_file(
                        f"ERROR: Failed to parse and calculate AUC for {txt_file_path}. Error: {e}",
                        root_dir=root_directory
                    )

            # ----------------------------------------------------
            # 2. æ±‡æ€»äº”æŠ˜ç»“æœå¹¶è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ (è¿™éƒ¨åˆ†é€»è¾‘æ˜¯æ ¸å¿ƒä¿®æ”¹)
            # ----------------------------------------------------
            
            if not fold_results:
                log_missing_file(
                    f"Skipping {dataset_name}/{model_name}: No successful folds found.",
                    root_dir=root_directory
                )
                continue
                
            # å°†äº”æŠ˜ç»“æœåˆ—è¡¨è½¬æ¢ä¸º DataFrame
            df_folds = pd.DataFrame(fold_results)
            
            # ä¿å­˜äº”æŠ˜çš„ overall_auc_info æ±‡æ€» CSV
            df_folds.to_csv(output_csv_path, index=False)
            print(f"âœ… Saved fold summary CSV to: {output_csv_path}")

            # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡çš„äº”æŠ˜å‡å€¼å’Œäº”æŠ˜æ ‡å‡†å·®
            mean_stats = df_folds.mean(numeric_only=True).rename(lambda x: f'{x}_mean')
            # ç¡®ä¿ä½¿ç”¨ ddof=1 è®¡ç®—æ ·æœ¬æ ‡å‡†å·®
            std_stats = df_folds.std(ddof=1, numeric_only=True).rename(lambda x: f'{x}_std') 
            
            # æ•´åˆç»Ÿè®¡ç»“æœ
            df_stats = pd.concat([mean_stats, std_stats]).to_frame().T
            
            # è®¡ç®—ä¸å…¬å¹³æ€§æŒ‡æ ‡çš„å‡å€¼ (éœ€è¦ç”¨åˆ°å‡å€¼ç»“æœ)
            mean_auc = df_stats['student_auc_mean_mean'].iloc[0]
            mean_range = df_stats['student_auc_range_mean'].iloc[0]
            mean_std = df_stats['student_auc_std_mean'].iloc[0]
            
            # è®¡ç®—ä¸å…¬å¹³æ€§æŒ‡æ ‡çš„å‡å€¼
            unfairness_mean = calculate_unfairness(mean_auc, mean_range, mean_std)

            # å°†ç»Ÿè®¡ç»“æœä¿å­˜åˆ°æ¨¡å‹æ–‡ä»¶å¤¹ä¸‹çš„TXTæ–‡ä»¶
            with open(output_txt_path, 'w', encoding='utf-8') as f:
                f.write(f"--- {dataset_name}/{model_name} Final Stats ---\n")
                
                # å†™å…¥æ‰€æœ‰æŒ‡æ ‡çš„å‡å€¼å’Œæ ‡å‡†å·®
                for col in df_stats.columns:
                    f.write(f"{col}: {df_stats[col].iloc[0]:.6f}\n")
                
                # å†™å…¥ä¸å…¬å¹³æ€§æŒ‡æ ‡å‡å€¼
                f.write(f"\nunfairness_metric_mean: {unfairness_mean:.6f}\n")
            
            print(f"âœ… Saved final stats TXT to: {output_txt_path}")

            # ----------------------------------------------------
            # 3. å‡†å¤‡æœ€ç»ˆ baseline è¡¨æ•°æ® (å·²ä¿®æ”¹ï¼Œå¢åŠ äº†æ‰€æœ‰æŒ‡æ ‡çš„äº”æŠ˜æ ‡å‡†å·®)
            # ----------------------------------------------------
            
            # æå–å…³é”®å‡å€¼å’Œæ ‡å‡†å·®æŒ‡æ ‡ï¼Œç”¨äº baseline å¤§è¡¨
            baseline_entry = {
                'Dataset': dataset_name,
                'Model': model_name,
                
                # === åŸæœ‰æŒ‡æ ‡ï¼ˆä¿æŒä¸åŠ¨ï¼‰===
                'overall_dataset_auc': df_stats.get('overall_dataset_auc_mean', np.nan),
                'overall_dataset_auc_std': df_stats.get('overall_dataset_auc_std', np.nan),
                
                'student_auc_mean': df_stats.get('student_auc_mean_mean', np.nan),
                'student_auc_mean_std': df_stats.get('student_auc_mean_std', np.nan),
                
                'student_auc_std': df_stats.get('student_auc_std_mean', np.nan),
                'student_auc_std_std': df_stats.get('student_auc_std_std', np.nan),
                
                'student_auc_max': df_stats.get('student_auc_max_mean', np.nan),
                'student_auc_max_std': df_stats.get('student_auc_max_std', np.nan),
                
                'student_auc_min': df_stats.get('student_auc_min_mean', np.nan),
                'student_auc_min_std': df_stats.get('student_auc_min_std', np.nan),
                
                'student_auc_range': df_stats.get('student_auc_range_mean', np.nan),
                'student_auc_range_std': df_stats.get('student_auc_range_std', np.nan),
                
                # === æ–°å¢æŒ‡æ ‡ï¼šè‡ªåŠ¨æ˜ å°„ï¼ˆæœ€ä¼˜é›…ï¼ï¼‰===
                'average_auc': df_stats.get('student_auc_average_auc_mean', np.nan),
                'average_auc_std': df_stats.get('student_auc_average_auc_std', np.nan),
                
                'gini_coefficient': df_stats.get('student_auc_gini_coefficient_mean', np.nan),
                'gini_coefficient_std': df_stats.get('student_auc_gini_coefficient_std', np.nan),
                
                'eawi_alpha_10': df_stats.get('student_auc_eawi_alpha_10_mean', np.nan),
                'eawi_alpha_10_std': df_stats.get('student_auc_eawi_alpha_10_std', np.nan),
                
                'eawi_alpha_20': df_stats.get('student_auc_eawi_alpha_20_mean', np.nan),
                'eawi_alpha_20_std': df_stats.get('student_auc_eawi_alpha_20_std', np.nan),
                
                'eawi_alpha_30': df_stats.get('student_auc_eawi_alpha_30_mean', np.nan),
                'eawi_alpha_30_std': df_stats.get('student_auc_eawi_alpha_30_std', np.nan),
                
                # === ä¸å…¬å¹³æ€§æŒ‡æ ‡ ===
                'unfairness_metric': unfairness_mean,
                
                'Folds_Present': ','.join(map(str, sorted(successful_folds)))
            }
            final_baseline_data.append(baseline_entry)


    # ----------------------------------------------------
    # 4. ç”Ÿæˆæœ€ç»ˆ Baseline å¤§è¡¨ CSV (ä¿æŒä¸å˜)
    # ----------------------------------------------------
    
    if final_baseline_data:
        df_baseline = pd.DataFrame(final_baseline_data)
        
        # æ’åºå’Œé‡æ’å­—æ®µ
        final_cols = ['Dataset', 'Model'] + FINAL_BASELINE_COLUMNS + ['Folds_Present']
        df_baseline = df_baseline[final_cols]

        # æœ€ç»ˆä¿å­˜è·¯å¾„åœ¨æ ¹ç›®å½•
        df_baseline.to_csv(baseline_output_path, index=False)
        print("\n=======================================================")
        print(f"ğŸ”¥ FINAL BASELINE TABLE SAVED TO: {baseline_output_path}")
        print("=======================================================")
        return df_baseline
    else:
        print("âŒ No valid results were processed to create the final baseline table.")
        return pd.DataFrame()

# --- æœ€ç»ˆæ‰§è¡Œ ---
analysis_root_dir = './'
final_baseline_table = analyze_all_results(analysis_root_dir)
print(final_baseline_table)